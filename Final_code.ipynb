{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaghyjuli/RL/blob/main/Final_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9jpX4XrBWrB"
      },
      "outputs": [],
      "source": [
        "## based on https://github.com/DanielPalaio/LunarLander-v2_DeepRL\n",
        "\n",
        "!pip install gym[box2d]==0.17\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIaVadfNEluj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Constants for DQN Agent\n",
        "\"\"\"\n",
        "\n",
        "epsilon = 1 # amount of exploration\n",
        "gamma = .99 # temporal discount\n",
        "learning_rate = 0.001 # alpha\n",
        "tau = 0.01 # update rate of target network\n",
        "\n",
        "memory_buffer_size= 500000 # number of experiences possibly stored at once\n",
        "batch_size = 64 # number of samples in each batch\n",
        "n_actions = 4\n",
        "state_dim = 8\n",
        "n_episodes = 500\n",
        "\n",
        "from enum import Enum\n",
        "class DecayType(Enum):\n",
        "    EXPONENTIAL = 0\n",
        "    LINEAR = 1\n",
        "\n",
        "epsilon_decay_type = DecayType.LINEAR\n",
        "\n",
        "lin_epsilon_decay_param = 0.001 # constant subtraction\n",
        "exp_epsilon_decay_param = 0.99 # constant multiplication\n",
        "min_eps = 0.01 # final epsilon value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lyunh87A_yN"
      },
      "outputs": [],
      "source": [
        "class MemoryBuffer:\n",
        "    \"\"\"\n",
        "    Stores experiences (state, action, reward, new_state, done) for i.i.d. assumption during learning\n",
        "    \"\"\" \n",
        "    def __init__(self):\n",
        "        self.states = np.zeros((memory_buffer_size, 8), np.float64)\n",
        "        self.actions = np.zeros(memory_buffer_size, np.intc)\n",
        "        self.rewards = np.zeros(memory_buffer_size, np.float64)\n",
        "        self.new_states = np.zeros((memory_buffer_size, 8), np.float64)\n",
        "        self.dones = np.zeros(memory_buffer_size, np.bool_)\n",
        "\n",
        "        self.head = 0\n",
        "\n",
        "    def write(self, state, action, reward, new_state, done):\n",
        "        index = self.head % memory_buffer_size\n",
        "\n",
        "        self.states[index] = state\n",
        "        self.actions[index] = action\n",
        "        self.rewards[index] = reward\n",
        "        self.new_states[index] = new_state\n",
        "        self.dones[index] = done\n",
        "\n",
        "        self.head += 1\n",
        "\n",
        "    def sample(self):\n",
        "        sample = np.random.choice(min(self.head, memory_buffer_size), batch_size)\n",
        "\n",
        "        return (self.states[sample],\n",
        "                self.actions[sample],\n",
        "                self.rewards[sample],\n",
        "                self.new_states[sample],\n",
        "                self.dones[sample])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkLj9FAcGYjz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.activations import relu, linear\n",
        "from keras import Sequential\n",
        "\n",
        "class DQN(Sequential):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.add(keras.layers.Dense(256, input_dim=state_dim, activation=relu))\n",
        "        self.add(keras.layers.Dense(256, activation=relu))\n",
        "        self.add(keras.layers.Dense(n_actions, activation=linear))\n",
        "        self.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=learning_rate ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz2_IxJ9A3rj"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Agent that plays and learns.\n",
        "    It has access to the memory buffer and stores the DQN and DQN target.\n",
        "    \"\"\" \n",
        "    def __init__(self, buffer):\n",
        "        self.epsilon = epsilon\n",
        "        self.buffer = buffer\n",
        "        self.dqn = DQN()\n",
        "        self.dqn_target = DQN()\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(n_actions)\n",
        "        state = np.array([state])\n",
        "        qs = self.dqn(state)\n",
        "        return np.argmax(qs)\n",
        "\n",
        "    def update_target(self):\n",
        "        new_weights = []\n",
        "        target_weights = self.dqn_target.get_weights()\n",
        "        main_weights = self.dqn.get_weights()\n",
        "        for target_weight, main_weight in zip(target_weights, main_weights):\n",
        "            new_weights.append((1 - tau) * target_weight + tau * main_weight)\n",
        "        self.dqn_target.set_weights(new_weights)\n",
        "\n",
        "    def update(self):\n",
        "        states, actions, rewards, new_states, dones = self.buffer.sample()\n",
        "        # we change y only in the (s, a) positions where experiences are available\n",
        "        y = np.copy(self.dqn(states))\n",
        "        next_qs = self.dqn_target(new_states)\n",
        "        max_next_qs = np.amax(next_qs, axis=1)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            # times done to avoid learning over final experiences\n",
        "            y[idx, actions[idx]] = rewards[idx] + (1 - dones[idx]) * gamma * max_next_qs[idx]\n",
        "\n",
        "        self.dqn.train_on_batch(states, y)\n",
        "        self.update_target()\n",
        "\n",
        "        if self.epsilon > min_eps:\n",
        "            if epsilon_decay_type == DecayType.LINEAR:\n",
        "                self.epsilon -= lin_epsilon_decay_param\n",
        "            elif epsilon_decay_type == DecayType.EXPONENTIAL:\n",
        "                self.epsilon *= exp_epsilon_decay_param\n",
        "            else:\n",
        "                print(\"please choose decay type\")\n",
        "                exit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SARSAAgent():\n",
        "  \"\"\"\n",
        "  Agent that learns using the SARSA algorithm. \n",
        "  \"\"\"\n",
        "  def __init__(self, n_actions):\n",
        "    self.gamma = 0.99                               # discount\n",
        "\n",
        "    self.num_bins = 3                               # number of bins for discretizing continuous state variables\n",
        "    self.bins2D = []                                # num_bins bins for each of the 6 continuous variables\n",
        "    self.init_bins()\n",
        "\n",
        "    self.n_actions = n_actions                               # number of actions\n",
        "    self.dim_state = (self.num_bins ** 6) * (2 ** 2)         # 6 continuous, 2 binary\n",
        "    self.Q = np.random.rand(self.dim_state, n_actions)       # initialize Q(s,a) table\n",
        "\n",
        "  def get_action(self, state, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "      return np.random.choice(range(self.n_actions))\n",
        "    else:\n",
        "      return np.argmax(self.Q[self.get_state_index(state)])\n",
        "\n",
        "  def q_update(self, prev_state, action, reward, new_state, alpha):\n",
        "    prev_state_idx = self.get_state_index(prev_state)\n",
        "    new_state_idx = self.get_state_index(new_state)\n",
        "    self.Q[prev_state_idx][action] += alpha * (reward + self.gamma * np.max(self.Q[new_state_idx]) - self.Q[prev_state_idx][action])\n",
        "\n",
        "  def init_bins(self):\n",
        "    state_min = [-1.01975346, -0.44636688, -2.29403067, -2.22342443, -4.9213028, -8.83636475]\n",
        "    state_max = [1.0239284, 1.8099494, 2.47327113, 0.62224495, 4.42566919, 8.98535538]\n",
        "    half_mid_range = 20\n",
        "    for i in range(6):\n",
        "      #mid_bound = (state_max[i] - state_min[i]) / half_mid_range\n",
        "      mid_bound = 0.05\n",
        "      left_bounds = [-float(\"inf\"), -mid_bound, mid_bound]\n",
        "      right_bounds = [-mid_bound, mid_bound, float(\"inf\")]\n",
        "      self.bins2D.append(pd.IntervalIndex.from_arrays(left_bounds, right_bounds, closed=\"neither\"))\n",
        "\n",
        "  def get_state_representation(self, state):\n",
        "    representation = [self.bins2D[i].get_loc(state[i]) for i in range(6)]\n",
        "    representation.append(int(state[6]))\n",
        "    representation.append(int(state[7]))\n",
        "    return representation\n",
        "\n",
        "  def get_state_index(self, state):\n",
        "    bases = [self.num_bins]*6 + [2, 2]\n",
        "    n = 0\n",
        "    for i in range(len(state) - 2):\n",
        "      n = (n + self.bins2D[i].get_loc(state[i])) * bases[i+1]\n",
        "    return (n + int(state[-2]))*2 + int(state[-1])"
      ],
      "metadata": {
        "id": "C8spU9-21SU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pqDYvm1A0Tr"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.twodim_base import triu_indices_from\n",
        "\"\"\"\n",
        "Main experiment loop for DQN Agent\n",
        "\"\"\" \n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "n_experiments = 5\n",
        "cum_rewards = []\n",
        "timesteps = []\n",
        "\n",
        "for experiment in range(n_experiments):\n",
        "\n",
        "  buffer = MemoryBuffer()\n",
        "  agent = DQNAgent(buffer)\n",
        "\n",
        "  cum_rewards_experiment = []\n",
        "  timesteps_experiment = []\n",
        "\n",
        "  for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    cum_reward_episode = 0\n",
        "    while True:\n",
        "      prev_state = state\n",
        "      action = agent.get_action(state)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      cum_reward_episode += reward\n",
        "      buffer.write(prev_state, action, reward, state, done)\n",
        "      agent.update()\n",
        "      if done:\n",
        "        print(f\"Episode {episode+1} - {t+1} timesteps, cum_reward = {cum_reward_episode} \\n\")\n",
        "        timesteps_experiment.append(t+1)\n",
        "        cum_rewards_experiment.append(cum_reward_episode)\n",
        "        break\n",
        "      t += 1\n",
        "\n",
        "  cum_rewards.append(cum_rewards_experiment)\n",
        "  timesteps.append(timesteps_experiment)\n",
        "  print(f\"Experiment {experiment+1} finished.\\n\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Main experiment loop for SARSA Agent\n",
        "\"\"\" \n",
        "\n",
        "def get_epsilon(episode_number):\n",
        "    if episode_number < 200:\n",
        "      return 0.5\n",
        "    if episode_number < 1000:\n",
        "      return 0.2\n",
        "    if episode_number < 1500:\n",
        "      return 0.1\n",
        "    if episode_number < 8000:\n",
        "      return 0.01\n",
        "    if episode_number < 9000:\n",
        "      return 0.001\n",
        "    return 0\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "n_experiments = 5\n",
        "n_episodes = 10000\n",
        "cum_rewards = []\n",
        "timesteps = []  \n",
        "\n",
        "for experiment in range(n_experiments):\n",
        "  agent = SARSAAgent(env.action_space.n)\n",
        "  cum_rewards_experiment = []\n",
        "  timesteps_experiment = []\n",
        "  for episode in range(n_episodes):\n",
        "      state = env.reset()\n",
        "      t = 0\n",
        "      cum_reward_episode = 0\n",
        "      alpha = (n_episodes - episode) / n_episodes\n",
        "      epsilon = get_epsilon(episode)\n",
        "      while True:\n",
        "        prev_state = state\n",
        "        prev_state_idx = agent.get_state_index(prev_state)\n",
        "        action = agent.get_action(state, epsilon)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        cum_reward_episode += reward\n",
        "        agent.q_update(prev_state, action, reward, state, alpha)\n",
        "        state_idx = agent.get_state_index(state)\n",
        "        if done:\n",
        "            print(f\"Episode {episode+1} - {t+1} timesteps, cum_reward = {cum_reward_episode} \\n\")\n",
        "            timesteps_experiment.append(t+1)\n",
        "            cum_rewards_experiment.append(cum_reward_episode)\n",
        "            break\n",
        "        t += 1\n",
        "\n",
        "  cum_rewards.append(cum_rewards_experiment)\n",
        "  timesteps.append(timesteps_experiment)\n",
        "  print(f\"Experiment {experiment+1} finished.\\n\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "e9-XOlYx105s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final_code.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}