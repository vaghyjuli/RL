{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Q-Leaning on the Lunar Lander V2 Problem (OpenAI Gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install gym[box2d]==0.17\n",
        "\n",
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIaVadfNEluj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Constants for DQN Agent\n",
        "\"\"\"\n",
        "\n",
        "epsilon = 1 # amount of exploration\n",
        "gamma = .99 # temporal discount\n",
        "learning_rate = 0.001 # alpha\n",
        "tau = 0.01 # update rate of target network\n",
        "\n",
        "memory_buffer_size= 500000 # number of experiences possibly stored at once\n",
        "batch_size = 64 # number of samples in each batch\n",
        "n_actions = 4\n",
        "state_dim = 8\n",
        "n_experiments = 5\n",
        "n_episodes_dqn = 500\n",
        "\n",
        "from enum import Enum\n",
        "class DecayType(Enum):\n",
        "    EXPONENTIAL = 0\n",
        "    LINEAR = 1\n",
        "\n",
        "epsilon_decay_type = DecayType.LINEAR\n",
        "\n",
        "lin_epsilon_decay_param = 0.002 # constant subtraction\n",
        "exp_epsilon_decay_param = 0.99 # constant multiplication\n",
        "min_eps = 0.01 # final epsilon value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lyunh87A_yN"
      },
      "outputs": [],
      "source": [
        "class MemoryBuffer:\n",
        "    \"\"\"\n",
        "    Stores experiences (state, action, reward, new_state, done) for i.i.d. assumption during learning\n",
        "    \"\"\" \n",
        "    def __init__(self):\n",
        "        self.states = np.zeros((memory_buffer_size, 8), np.float64)\n",
        "        self.actions = np.zeros(memory_buffer_size, np.intc)\n",
        "        self.rewards = np.zeros(memory_buffer_size, np.float64)\n",
        "        self.new_states = np.zeros((memory_buffer_size, 8), np.float64)\n",
        "        self.dones = np.zeros(memory_buffer_size, np.bool_)\n",
        "\n",
        "        self.head = 0\n",
        "\n",
        "    def write(self, state, action, reward, new_state, done):\n",
        "        index = self.head % memory_buffer_size\n",
        "\n",
        "        self.states[index] = state\n",
        "        self.actions[index] = action\n",
        "        self.rewards[index] = reward\n",
        "        self.new_states[index] = new_state\n",
        "        self.dones[index] = done\n",
        "\n",
        "        self.head += 1\n",
        "\n",
        "    def sample(self):\n",
        "        sample = np.random.choice(min(self.head, memory_buffer_size), batch_size)\n",
        "\n",
        "        return (self.states[sample],\n",
        "                self.actions[sample],\n",
        "                self.rewards[sample],\n",
        "                self.new_states[sample],\n",
        "                self.dones[sample])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkLj9FAcGYjz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.activations import relu, linear\n",
        "from keras import Sequential\n",
        "\n",
        "class DQN(Sequential):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.add(keras.layers.Dense(256, input_dim=state_dim, activation=relu))\n",
        "        self.add(keras.layers.Dense(256, activation=relu))\n",
        "        self.add(keras.layers.Dense(n_actions, activation=linear))\n",
        "        self.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=learning_rate ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz2_IxJ9A3rj"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    Agent that plays and learns.\n",
        "    It has access to the memory buffer and stores the DQN and DQN target.\n",
        "    \"\"\" \n",
        "    def __init__(self, buffer):\n",
        "        self.epsilon = epsilon\n",
        "        self.buffer = buffer\n",
        "        self.dqn = DQN()\n",
        "        self.dqn_target = DQN()\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(n_actions)\n",
        "        state = np.array([state])\n",
        "        qs = self.dqn(state)\n",
        "        return np.argmax(qs)\n",
        "\n",
        "    def update_target(self):\n",
        "        new_weights = []\n",
        "        target_weights = self.dqn_target.get_weights()\n",
        "        main_weights = self.dqn.get_weights()\n",
        "        for target_weight, main_weight in zip(target_weights, main_weights):\n",
        "            new_weights.append((1 - tau) * target_weight + tau * main_weight)\n",
        "        self.dqn_target.set_weights(new_weights)\n",
        "\n",
        "    def update(self):\n",
        "        states, actions, rewards, new_states, dones = self.buffer.sample()\n",
        "        # we change y only in the (s, a) positions where experiences are available\n",
        "        y = np.copy(self.dqn(states))\n",
        "        next_qs = self.dqn_target(new_states)\n",
        "        max_next_qs = np.amax(next_qs, axis=1)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            # times done to avoid learning over final experiences\n",
        "            y[idx, actions[idx]] = rewards[idx] + (1 - dones[idx]) * gamma * max_next_qs[idx]\n",
        "\n",
        "        self.dqn.train_on_batch(states, y)\n",
        "        self.update_target()\n",
        "\n",
        "        if self.epsilon > min_eps:\n",
        "            if epsilon_decay_type == DecayType.LINEAR:\n",
        "                self.epsilon -= lin_epsilon_decay_param\n",
        "            elif epsilon_decay_type == DecayType.EXPONENTIAL:\n",
        "                self.epsilon *= exp_epsilon_decay_param\n",
        "            else:\n",
        "                print(\"please choose decay type\")\n",
        "                exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pqDYvm1A0Tr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Main experiment loop for DQN Agent\n",
        "\"\"\" \n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "cum_rewards, timesteps, epsilons = [], [], []\n",
        "\n",
        "for experiment in range(n_experiments):\n",
        "\n",
        "  buffer = MemoryBuffer()\n",
        "  agent = DQNAgent(buffer)\n",
        "\n",
        "  cum_rewards_experiment = []\n",
        "  timesteps_experiment = []\n",
        "\n",
        "  for episode in range(n_episodes_dqn):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    cum_reward_episode = 0\n",
        "    while True:\n",
        "      prev_state = state\n",
        "      action = agent.get_action(state)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      cum_reward_episode += reward\n",
        "      buffer.write(prev_state, action, reward, state, done)\n",
        "      agent.update()\n",
        "      if done:\n",
        "        print(f\"Episode {episode+1} - {t+1} timesteps, cum_reward = {cum_reward_episode} \\n\")\n",
        "        timesteps_experiment.append(t+1)\n",
        "        cum_rewards_experiment.append(cum_reward_episode)\n",
        "        if experiment == 0:\n",
        "          epsilons.append(agent.epsilon)\n",
        "        break\n",
        "      t += 1\n",
        "\n",
        "  cum_rewards.append(cum_rewards_experiment)\n",
        "  timesteps.append(timesteps_experiment)\n",
        "  print(f\"Experiment {experiment+1} finished.\\n\")\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final_code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "5ee9078191084116e2e52cbdabed265951dcb065f749808b9a3dd11faeab9018"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
