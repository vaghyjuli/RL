{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Continuous2_Lunar_Lander.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaRdXiZbs3SZpuml1DOTDf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaghyjuli/RL/blob/main/Continuous2_Lunar_Lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF2kYReWB7yE",
        "outputId": "c3d10c36-571e-40f3-b836-c073c0b5abf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay==0.2.*) (1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "metadata": {
        "id": "NCDIX8MWB9Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.activations import relu, linear"
      ],
      "metadata": {
        "id": "8yesxgTSCAKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1\n",
        "gamma = .99\n",
        "batch_size = 64\n",
        "min_eps = 0.01\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "kUL2WRO8GWW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Sequential):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.epsilon = epsilon\n",
        "    self.add(keras.layers.Dense(64, input_dim=8, activation=relu))\n",
        "    self.add(keras.layers.Dense(64, activation=relu))\n",
        "    self.add(keras.layers.Dense(4, activation=linear))\n",
        "    self.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=learning_rate ))\n",
        "\n",
        "  def get_action(self, state):\n",
        "      pass\n",
        "\n",
        "  def replay_experiences(self, memory):\n",
        "    if len(memory) >= batch_size:\n",
        "        sample_choices = np.array(memory)\n",
        "        mini_batch_index = np.random.choice(len(sample_choices), batch_size)\n",
        "        #batch = random.sample(memory, batch_size)\n",
        "        states = []\n",
        "        actions = []\n",
        "        next_states = []\n",
        "        rewards = []\n",
        "        finishes = []\n",
        "        for index in mini_batch_index:\n",
        "            states.append(memory[index][0])\n",
        "            actions.append(memory[index][1])\n",
        "            next_states.append(memory[index][2])\n",
        "            rewards.append(memory[index][3])\n",
        "            finishes.append(memory[index][4])\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        next_states = np.array(next_states)\n",
        "        rewards = np.array(rewards)\n",
        "        finishes = np.array(finishes)\n",
        "        states = np.squeeze(states)\n",
        "        next_states = np.squeeze(next_states)\n",
        "        q_vals_next_state = self.predict_on_batch(next_states)\n",
        "        q_vals_target = self.predict_on_batch(states)\n",
        "        max_q_values_next_state = np.amax(q_vals_next_state, axis=1)\n",
        "        q_vals_target[np.arange(batch_size), actions] = rewards + gamma * (max_q_values_next_state) * (1 - finishes)\n",
        "        self.fit(states, q_vals_target, verbose=0)\n",
        "        global epsilon\n",
        "        if epsilon > min_eps:\n",
        "            epsilon *= 0.996"
      ],
      "metadata": {
        "id": "9gCD7yYkGZuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = []\n",
        "model = Model()\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "# env.seed(0)\n",
        "num_episodes = 400\n",
        "np.random.seed(0)\n",
        "scores  = []\n",
        "for i in range(num_episodes+1):\n",
        "    score = 0\n",
        "    state = env.reset()\n",
        "    finished = False\n",
        "    if i != 0 and i % 50 == 0:\n",
        "        model.save(\".\\saved_models\\model_\"+str(i)+\"_episodes.h5\")\n",
        "    for j in range(3000):\n",
        "        state = np.reshape(state, (1, 8))\n",
        "        if np.random.random() <= epsilon:\n",
        "            action =  np.random.choice(4)\n",
        "        else:\n",
        "            action_values = model.predict(state)\n",
        "            action = np.argmax(action_values[0])\n",
        "\n",
        "        #env.render()\n",
        "        next_state, reward, finished, metadata = env.step(action)\n",
        "        next_state = np.reshape(next_state, (1, 8))\n",
        "        memory.append((state, action, next_state, reward, finished))\n",
        "        model.replay_experiences(memory)\n",
        "        score += reward\n",
        "        state = next_state\n",
        "        if finished:\n",
        "            scores.append(score)\n",
        "            print(\"Episode = {}, Score = {}, Avg_Score = {}\".format(i, score, np.mean(scores[-100:])))\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyASyQiiC1vp",
        "outputId": "240017cc-64c0-42e6-8082-030f9e3e7cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode = 0, Score = -90.24569945676006, Avg_Score = -90.24569945676006\n",
            "Episode = 1, Score = -202.3822027228816, Avg_Score = -146.31395108982082\n",
            "Episode = 2, Score = -212.05975184757034, Avg_Score = -168.22921800907065\n",
            "Episode = 3, Score = -494.71907397606475, Avg_Score = -249.85168200081918\n",
            "Episode = 4, Score = -360.72299574761183, Avg_Score = -272.0259447501777\n",
            "Episode = 5, Score = -224.13026553395676, Avg_Score = -264.0433315474742\n",
            "Episode = 6, Score = -108.73899158696996, Avg_Score = -241.8569972674022\n",
            "Episode = 7, Score = -192.82670503482765, Avg_Score = -235.72821073833035\n",
            "Episode = 8, Score = -253.95123929364232, Avg_Score = -237.75299168892056\n",
            "Episode = 9, Score = -206.02370723673442, Avg_Score = -234.58006324370194\n",
            "Episode = 10, Score = -173.54685324987793, Avg_Score = -229.03158960789978\n",
            "Episode = 11, Score = -320.4812640058092, Avg_Score = -236.65239580772558\n",
            "Episode = 12, Score = -108.63570696493953, Avg_Score = -226.8049582044343\n",
            "Episode = 13, Score = -262.07047703340106, Avg_Score = -229.3239238350748\n",
            "Episode = 14, Score = -284.95161443170167, Avg_Score = -233.0324365415166\n",
            "Episode = 15, Score = -216.18537400334444, Avg_Score = -231.97949513288083\n",
            "Episode = 16, Score = -182.64826694320328, Avg_Score = -229.07765818054688\n",
            "Episode = 17, Score = -190.8484027166797, Avg_Score = -226.95381065477648\n",
            "Episode = 18, Score = -145.14210205842556, Avg_Score = -222.6479312549685\n",
            "Episode = 19, Score = -191.42753205011837, Avg_Score = -221.086911294726\n",
            "Episode = 20, Score = -159.60472249294506, Avg_Score = -218.15918801845072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kernel_size = 200\n",
        "kernel = np.ones(kernel_size) / kernel_size\n",
        "y = np.convolve(cum_rewards, kernel, mode=\"valid\")\n",
        "\n",
        "plt.plot(y, color=\"red\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o3f81rIcJY_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
