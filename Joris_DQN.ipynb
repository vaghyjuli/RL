{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Joris DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwEsD2+yMTEJRCH/BGrLKz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaghyjuli/RL/blob/main/Joris_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## based on https://github.com/DanielPalaio/LunarLander-v2_DeepRL\n",
        "\n",
        "\n",
        "!pip install gym[box2d]==0.17\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9jpX4XrBWrB",
        "outputId": "d65ea67d-6558-4fa0-c329-7b52e46b2657"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[box2d]==0.17 in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle~=1.3.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.3.0)\n",
            "Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1 # amount of exploration\n",
        "gamma = .99 # temporal discount\n",
        "batch_size = 64\n",
        "min_eps = 0.01\n",
        "update_rate = 120 # update rate of target network\n",
        "learning_rate = 0.001\n",
        "memory_buffer_size= 50000\n",
        "n_actions = 4\n",
        "input_dims= 8\n",
        "num_episodes = 1000\n",
        "\n",
        "from enum import Enum\n",
        "class DecayType(Enum):\n",
        "    EXPONENTIAL = 0\n",
        "    LINEAR = 1\n",
        "\n",
        "epsilon_decay_type = DecayType.LINEAR\n",
        "\n",
        "lin_epsilon_decay_param = 0.001\n",
        "exp_epsilon_decay_param = 0.99"
      ],
      "metadata": {
        "id": "pIaVadfNEluj"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemoryBuffer:\n",
        "    def __init__(self):\n",
        "        self.states = np.zeros((memory_buffer_size, 8), np.float64)\n",
        "        self.actions = np.zeros(memory_buffer_size, np.intc)\n",
        "        self.rewards = np.zeros(memory_buffer_size, np.float64)\n",
        "        self.new_states = np.zeros((memory_buffer_size, 8), np.float64)\n",
        "        self.dones = np.zeros(memory_buffer_size, np.bool_)\n",
        "\n",
        "        self.head = 0\n",
        "\n",
        "    def write(self, state, action, reward, new_state, done):\n",
        "        index = self.head % memory_buffer_size\n",
        "\n",
        "        self.states[index] = state\n",
        "        self.actions[index] = action\n",
        "        self.rewards[index] = reward\n",
        "        self.new_states[index] = new_state\n",
        "        self.dones[index] = done\n",
        "\n",
        "        self.head += 1\n",
        "\n",
        "    def sample(self):\n",
        "        sample = np.random.choice(min(self.head, memory_buffer_size), batch_size)\n",
        "\n",
        "        return (self.states[sample],\n",
        "                self.actions[sample],\n",
        "                self.rewards[sample],\n",
        "                self.new_states[sample],\n",
        "                self.dones[sample])"
      ],
      "metadata": {
        "id": "7lyunh87A_yN"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.activations import relu, linear\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "class DQN(tf.keras.Sequential):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.add(keras.layers.Dense(256, input_dim=8, activation=relu))\n",
        "    self.add(keras.layers.Dense(256, activation=relu))\n",
        "    self.add(keras.layers.Dense(n_actions, activation=linear))\n",
        "    self.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=learning_rate ))"
      ],
      "metadata": {
        "id": "kkLj9FAcGYjz"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, buffer):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.step_counter = 0\n",
        "        self.buffer = buffer\n",
        "        self.q_net = DQN()\n",
        "        self.q_target_net = DQN()\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(n_actions)\n",
        "        state = np.array([state])\n",
        "        action_values = self.q_net(state)\n",
        "        return np.argmax(action_values)\n",
        "\n",
        "    def update(self):\n",
        "        if self.step_counter % update_rate == 0:\n",
        "            self.q_target_net.set_weights(self.q_net.get_weights())\n",
        "\n",
        "        state_batch, action_batch, reward_batch, new_state_batch, done_batch = \\\n",
        "            self.buffer.sample()\n",
        "\n",
        "        q_predicted = self.q_net(state_batch)\n",
        "        q_next = self.q_target_net(new_state_batch)\n",
        "        q_max_next = tf.math.reduce_max(q_next, axis=1, keepdims=True).numpy()\n",
        "        q_target = np.copy(q_predicted)\n",
        "\n",
        "        for idx in range(done_batch.shape[0]):\n",
        "            target_q_val = reward_batch[idx]\n",
        "            if not done_batch[idx]:\n",
        "                target_q_val += gamma*q_max_next[idx]\n",
        "            q_target[idx, action_batch[idx]] = target_q_val\n",
        "        self.q_net.train_on_batch(state_batch, q_target)\n",
        "        if self.epsilon > min_eps:\n",
        "            if epsilon_decay_type == DecayType.LINEAR:\n",
        "                self.epsilon -= lin_epsilon_decay_param\n",
        "            elif epsilon_decay_type == DecayType.EXPONENTIAL:\n",
        "                self.epsilon *= exp_epsilon_decay_param\n",
        "            else:\n",
        "                print(\"please choose decay type\")\n",
        "                exit()\n",
        "        self.step_counter += 1\n"
      ],
      "metadata": {
        "id": "Nz2_IxJ9A3rj"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pqDYvm1A0Tr",
        "outputId": "73b4ea02-890a-4bdb-f6fa-e39f2db8230a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode = 0, Score = -118.78760412210974, Avg_Score = -118.78760412210974\n",
            "Episode = 1, Score = -220.2165029024912, Avg_Score = -169.50205351230045\n",
            "Episode = 2, Score = -140.30938490230898, Avg_Score = -159.77116397563663\n",
            "Episode = 3, Score = -166.83142137170165, Avg_Score = -161.53622832465288\n"
          ]
        }
      ],
      "source": [
        "###\n",
        "# Main loop\n",
        "###\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "buffer = MemoryBuffer()\n",
        "agent = Agent(env, buffer)\n",
        "\n",
        "episodes = range(num_episodes)\n",
        "scores, timesteps, epsilons = [], [], []\n",
        "for i in episodes:\n",
        "    score = 0\n",
        "    t = 1\n",
        "    state = env.reset()\n",
        "    for t in range(100000):\n",
        "        action = agent.get_action(state)\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        score += reward\n",
        "        buffer.write(state, action, reward, new_state, done)\n",
        "        state = new_state\n",
        "        agent.update()\n",
        "        if done:\n",
        "            scores.append(score)\n",
        "            epsilons.append(agent.epsilon)\n",
        "            timesteps.append(t)\n",
        "            print(\"Episode = {}, Score = {}, Avg_Score = {}\".format(i, score, np.mean(scores[-100:])))\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kernel_size = 20\n",
        "kernel = np.ones(kernel_size) / kernel_size\n",
        "cr = np.convolve(scores, kernel, mode=\"valid\")\n",
        "tst = np.convolve(timesteps, kernel, mode=\"valid\")\n",
        "eps = np.convolve(epsilons, kernel, mode=\"valid\")\n",
        "\n",
        "rew = plt.figure(1)\n",
        "target = [200 for _ in episodes]\n",
        "plt.plot(episodes, target, color='red', linewidth=2, linestyle='dashed',\n",
        "          label='Solved Requirement')\n",
        "plt.plot(episodes, scores, color=\"black\", linewidth=2, label='Score')\n",
        "plt.suptitle('DQN')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Cumulative reward')\n",
        "plt.show()\n",
        "\n",
        "timest = plt.figure(2)\n",
        "plt.plot(tst, color=\"blue\")\n",
        "plt.suptitle('DQN')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Number of time steps')\n",
        "timest.show()\n",
        "\n",
        "timest = plt.figure(3)\n",
        "plt.plot(eps, color=\"green\", label='Epsilon')\n",
        "plt.suptitle('DQN')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Epsilon')\n",
        "timest.show()\n",
        "\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Bt8OC4dYMMuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Testing\n",
        "###\n",
        "\n",
        "scores, timesteps, epsilons = [], [], []\n",
        "for i in episodes:\n",
        "    score = 0\n",
        "    t = 1\n",
        "    state = env.reset()\n",
        "    for t in range(100000):\n",
        "        action = agent.get_action(state)\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        score += reward\n",
        "        state = new_state\n",
        "        if done:\n",
        "            scores.append(score)\n",
        "            epsilons.append(agent.epsilon)\n",
        "            timesteps.append(t)\n",
        "            print(\"Episode = {}, Score = {}, Avg_Score = {}\".format(i, score, np.mean(scores[-100:])))\n",
        "            break\n",
        "\n",
        "# Plot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kernel_size = 20\n",
        "kernel = np.ones(kernel_size) / kernel_size\n",
        "cr = np.convolve(scores, kernel, mode=\"valid\")\n",
        "tst = np.convolve(timesteps, kernel, mode=\"valid\")\n",
        "eps = np.convolve(epsilons, kernel, mode=\"valid\")\n",
        "\n",
        "rew = plt.figure(1)\n",
        "target = [200 for _ in episodes]\n",
        "plt.plot(episodes, target, color='red', linewidth=2, linestyle='dashed',\n",
        "          label='Solved Requirement')\n",
        "plt.plot(episodes, scores, color=\"black\", linewidth=2, label='Score')\n",
        "plt.suptitle('DQN')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Cumulative reward')\n",
        "plt.show()\n",
        "\n",
        "timest = plt.figure(2)\n",
        "plt.plot(tst, color=\"blue\")\n",
        "plt.suptitle('DQN')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Number of time steps')\n",
        "timest.show()\n",
        "\n",
        "timest = plt.figure(3)\n",
        "plt.plot(eps, color=\"green\", label='Epsilon')\n",
        "plt.suptitle('DQN')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Epsilon')\n",
        "timest.show()\n",
        "\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "IHUX6x-PMlo_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}