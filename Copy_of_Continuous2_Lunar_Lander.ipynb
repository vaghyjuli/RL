{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Continuous2_Lunar_Lander.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaghyjuli/RL/blob/main/Copy_of_Continuous2_Lunar_Lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[box2d]==0.17\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.activations import relu, linear"
      ],
      "metadata": {
        "id": "8yesxgTSCAKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c01f341-c7f1-4dfb-a3ee-5404126315b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym[box2d]==0.17\n",
            "  Downloading gym-0.17.0.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle~=1.3.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17) (1.3.0)\n",
            "Collecting box2d-py~=2.3.5\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.0-py3-none-any.whl size=1648707 sha256=79b820c4dd500b04add6a167949adb48574774d63f673c1aefde2dedd51a750d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/d6/e8/4c7bed6daf2593a6752cdd0b74906e59cf8c86a05a15ca77f0\n",
            "Successfully built gym\n",
            "Installing collected packages: gym, box2d-py\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed box2d-py-2.3.8 gym-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(tf.keras.Sequential):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.epsilon = 1\n",
        "    self.min_eps = 0.001\n",
        "    self.gamma = .99\n",
        "    self.batch_size = 64\n",
        "    self.learning_rate = 0.001\n",
        "\n",
        "    self.memory_size = 10000\n",
        "    self.memory = []\n",
        "    self.memory_number = 0\n",
        "\n",
        "    self.add(keras.layers.Dense(64, input_dim=8, activation=relu))\n",
        "    self.add(keras.layers.Dense(64, activation=relu))\n",
        "    self.add(keras.layers.Dense(4, activation=linear))\n",
        "    self.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
        "\n",
        "  def get_action(self, state):\n",
        "    if np.random.random() <= self.epsilon:\n",
        "      return np.random.choice(4)\n",
        "    else:\n",
        "      action_values = self.predict(state)\n",
        "      return np.argmax(action_values[0])\n",
        "\n",
        "  def remember(self, experience):\n",
        "    if self.memory_number < self.memory_size:\n",
        "      self.memory.append(experience)\n",
        "    else:\n",
        "      idx = self.memory_number % self.memory_size\n",
        "      self.memory[idx] = experience\n",
        "    self.memory_number += 1\n",
        "\n",
        "  def replay_experiences(self):\n",
        "    if len(self.memory) < self.batch_size:\n",
        "      return\n",
        "\n",
        "    mini_batch_index = np.random.choice(len(self.memory), self.batch_size)\n",
        "    #batch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "    states = np.squeeze(np.array([self.memory[idx][0] for idx in mini_batch_index]))\n",
        "    actions = np.array([self.memory[idx][1] for idx in mini_batch_index])\n",
        "    next_states = np.squeeze(np.array([self.memory[idx][2] for idx in mini_batch_index]))\n",
        "    rewards = np.array([self.memory[idx][3] for idx in mini_batch_index])\n",
        "    finishes = np.array([self.memory[idx][4] for idx in mini_batch_index])\n",
        "\n",
        "    q_vals_next_state = self.predict_on_batch(next_states)\n",
        "    q_vals_target = self.predict_on_batch(states)\n",
        "    max_q_values_next_state = np.amax(q_vals_next_state, axis=1)\n",
        "    q_vals_target[np.arange(self.batch_size), actions] = rewards + self.gamma * (max_q_values_next_state) * (1 - finishes)\n",
        "    self.fit(states, q_vals_target, verbose=0)\n",
        "    #self.train_on_batch(states, q_vals_target)\n",
        "    \n",
        "    if self.epsilon > self.min_eps:\n",
        "        self.epsilon *= 0.998"
      ],
      "metadata": {
        "id": "9gCD7yYkGZuX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent()\n",
        "env = gym.make('LunarLander-v2')\n",
        "# env.seed(0)\n",
        "# np.random.seed(0)\n",
        "num_episodes = 100\n",
        "scores  = []\n",
        "timesteps = []\n",
        "epsilons = []\n",
        "for i in range(num_episodes):\n",
        "  score = 0\n",
        "  t = 0\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  if i != 0 and i % 50 == 0:\n",
        "    agent.save(\".\\saved_models\\model_\"+str(i)+\"_episodes.h5\")\n",
        "  while True:\n",
        "    t += 1\n",
        "    state = np.reshape(state, (1, 8))\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, metadata = env.step(action)\n",
        "    next_state = np.reshape(next_state, (1, 8))\n",
        "    agent.remember((state, action, next_state, reward, done))\n",
        "    score += reward\n",
        "    state = next_state\n",
        "    agent.replay_experiences()\n",
        "    if done:\n",
        "      scores.append(score)\n",
        "      timesteps.append(t)\n",
        "      epsilons.append(agent.epsilon)\n",
        "      print(\"Episode = {}, Score = {}, Avg_Score = {}\".format(i, score, np.mean(scores[-100:])))\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyASyQiiC1vp",
        "outputId": "7b8d70fd-16cb-4309-b571-574493b52e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode = 0, Score = -109.82101375555632, Avg_Score = -109.82101375555632\n",
            "Episode = 1, Score = -101.70018917421578, Avg_Score = -105.76060146488605\n",
            "Episode = 2, Score = -277.5375733342171, Avg_Score = -163.01959208799641\n",
            "Episode = 3, Score = -305.9109014480598, Avg_Score = -198.74241942801225\n",
            "Episode = 4, Score = -80.67419212677277, Avg_Score = -175.12877396776435\n",
            "Episode = 5, Score = -43.93405278796576, Avg_Score = -153.2629871044646\n",
            "Episode = 6, Score = -63.97365149809842, Avg_Score = -140.50736773212657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kernel_size = 1\n",
        "kernel = np.ones(kernel_size) / kernel_size\n",
        "# cr = np.convolve(np.mean(scores, axis=0), kernel, mode=\"valid\")\n",
        "# tst = np.convolve(np.mean(timesteps, axis=0), kernel, mode=\"valid\")\n",
        "cr = np.convolve(scores, kernel, mode=\"valid\")\n",
        "tst = np.convolve(timesteps, kernel, mode=\"valid\")\n",
        "eps = np.convolve(epsilons, kernel, mode=\"valid\")\n",
        "\n",
        "rew = plt.figure(1)\n",
        "plt.plot(cr, color=\"red\")\n",
        "plt.suptitle('DQN')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Cumulative reward')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "timest = plt.figure(2)\n",
        "plt.plot(tst, color=\"blue\")\n",
        "plt.suptitle('DQN')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Number of time steps')\n",
        "timest.show()\n",
        "\n",
        "timest = plt.figure(3)\n",
        "plt.plot(eps, color=\"green\")\n",
        "plt.suptitle('DQN')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Epsilon')\n",
        "timest.show()"
      ],
      "metadata": {
        "id": "o3f81rIcJY_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d4MEo9c_AiR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}